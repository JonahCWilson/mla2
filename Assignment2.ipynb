{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "def load_occupancy_training():\n",
    "    enc = OrdinalEncoder()\n",
    "    \n",
    "    f = open('./datatraining.txt')\n",
    "    lines = f.readlines()[1:] #eliminate categories\n",
    "    lines = [line.strip().split(',') for line in lines]\n",
    "    f.close()\n",
    "    \n",
    "    X, y = [ls[:-1] for ls in lines],[ls[-1] for ls in lines]\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X)\n",
    "    y = [int(i) for i in y]\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def load_occupancy_test():\n",
    "    enc = OrdinalEncoder()\n",
    "    \n",
    "    f = open('./datatest.txt')\n",
    "    lines = f.readlines()[1:] #eliminate categories\n",
    "    lines = [line.strip().split(',') for line in lines]\n",
    "    f.close()\n",
    "    \n",
    "    X, y = [ls[:-1] for ls in lines],[ls[-1] for ls in lines]\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X)\n",
    "    y = [int(i) for i in y]\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def get_best_model(scores):\n",
    "    best = 0\n",
    "    best_index = 0\n",
    "    s = scores['test_score']\n",
    "    for i in range(len(s)):\n",
    "        if s[i] > best:\n",
    "            best = s[i]\n",
    "            best_index = i\n",
    "\n",
    "    best_model = scores['estimator'][best_index]\n",
    "    return best_model\n",
    "\n",
    "def get_best_score(scores):\n",
    "    best = 0\n",
    "    best_index = 0\n",
    "    s = scores['test_score']\n",
    "    for i in range(len(s)):\n",
    "        if s[i] > best:\n",
    "            best = s[i]\n",
    "            best_index = i\n",
    "        \n",
    "    return best, best_index\n",
    "\n",
    "occupancyTrainX, occupancyTrainY = load_occupancy_training()\n",
    "occupancyTestX, occupancyTestY = load_occupancy_test()\n",
    "\n",
    "\n",
    "digitData, digitTarget = load_digits(return_X_y=True)\n",
    "trainDigitX, testDigitX, trainDigitY, testDigitY = train_test_split(digitData, digitTarget, test_size=.35)\n",
    "\n",
    "occTrainX, occTestX, occTrainY, occTestY = train_test_split(occupancyTrainX, occupancyTrainY, test_size=.35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curve plotting function\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrose_reborn as mlrose\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Neural Networks\n",
    "\n",
    "nn = MLPClassifier()\n",
    "nn.fit(trainDigitX, trainDigitY)\n",
    "print(nn)\n",
    "plot_learning_curve(nn, \"Neural Network Training\", trainDigitX, trainDigitY)\n",
    "\n",
    "# Create Parameter Dict for GridSearch\n",
    "\n",
    "params = {\n",
    "    'activation': ['identity', 'tanh', 'relu', 'logistic'],\n",
    "    'hidden_layer_sizes': [(2,)],\n",
    "    'solver': ['adam'],\n",
    "    'early_stopping': [True, False],\n",
    "    'max_iter': [5000],\n",
    "    'random_state': [0]  \n",
    "    \n",
    "}\n",
    "\n",
    "nn = MLPClassifier()\n",
    "clf = GridSearchCV(nn, param_grid=params)\n",
    "plot_learning_curve(clf, \"Test\", trainDigitX, trainDigitY)\n",
    "clf.fit(trainDigitsX, trainDigitsY)\n",
    "clf.score(testDigitsX, testDigitsY)\n",
    "# digits_data, digits_target = load_digits(return_X_y=True)\n",
    "\n",
    "# d_train_X, d_test_X, d_train_y, d_test_y = train_test_split(digits_data, digits_target, test_size=.33, random_state=0, shuffle=True)\n",
    "# scores = cross_validate(nn, d_train_X, d_train_y, return_estimator=True)\n",
    "# bestModel = get_best_model(scores)\n",
    "# s = bestModel.predict(d_test_X)\n",
    "# print(sum(s == d_test_y)/len(d_test_y))\n",
    "\n",
    "# nn = MLPClassifier(**params)\n",
    "\n",
    "# occ_train_X, occ_test_X, occ_train_y, occ_test_y = train_test_split(occupancy_train_X, occupancy_train_y, test_size=.33, random_state=0, shuffle=True)\n",
    "# scores = cross_validate(nn, occ_train_X, occ_train_y, return_estimator=True)\n",
    "# bestModel = get_best_model(scores)\n",
    "# accuracy = sum(bestModel.predict(occ_test_X) == occ_test_y)/len(occ_test_y)\n",
    "# print(accuracy)\n",
    "# output = bestModel.predict(occupancy_test_X)\n",
    "\n",
    "# accuracy = sum(output == occupancy_test_y)/len(occupancy_test_y)\n",
    "# print(\"Occupancy Set Accuracy: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
